<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZL5EDDPNDM"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-ZL5EDDPNDM');
  </script>
  <meta charset="utf-8">
  <meta name="description"
        content="Viability-Preserving Passive Torque Control">
  <meta name="keywords" content="Robotics, Viability, Collision Avoidance, Physical Human-Robot Interaction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VPP-TC: Viability-Preserving Passive Torque Control</title>

  <!-- Thumbnail for social media sharing -->
  <meta property="og:image" content="media/thumbnail.jpg">

  <!-- Favicon -->
  <link rel="icon" href="media/vpptc.ico" type="image/x-icon">

  <script>
    window.dataLayer = window.dataLayer || [];
  </script>

  <script>
    function updateInTheWild() {
      var task = document.getElementById("inthewild-video-menu").value;

      console.log("updateInTheWild", task)

      var video = document.getElementById("inthewild-video");
      video.src = "media/videos/" + 
                  task + 
                  ".m4v"
      video.play();
    }

    function updateBimanual() {
      var task = document.getElementById("bimanual-video-menu").value;

      console.log("updateBimanual", task)

      var video = document.getElementById("bimanual-video");
      video.src = "media/videos/" + 
                  task + 
                  ".m4v"
      video.play();
    }

    function updateClothes() {
      var task = document.getElementById("clothes-video-menu").value;

      console.log("updateclothes", task)

      var img = document.getElementById("clothes-img");
      img.src = "media/fold-strategies/" + 
                  task + 
                  ".jpeg"

      var video = document.getElementById("clothes-video");
      video.src = "media/videos/fold-" + 
                  task + 
                  ".mp4"
      video.play();
    }
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <link rel="stylesheet" href="./static/source_serif_4.css">
  <link rel="stylesheet" href="./static/source_sans_3.css">  
  <link rel="stylesheet" href="./static/academicons.min.css">
  <link rel="stylesheet" href="./static/fontawesome/css/fontawesome.css">
  <link rel="stylesheet" href="./static/fontawesome/css/brands.css">
  <link rel="stylesheet" href="./static/fontawesome/css/light.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    .title-video{
      width: min(120vw, 1600px);
      margin: -2rem 0 1rem;
      position: relative;
      left: 50%;
      transform: translateX(-50%); /* 核心：视口居中 */
      overflow: hidden;
    }
    .title-video video{
      display: block;
      width: 100%;
      height: auto;
      object-fit: contain;
      clip-path: inset(0 0 2px 0);
    }
  </style>
  <style>
    /* Page background */
    html, body { background: #fefefe; }
    body { margin: 0; }

    /* Bulma 一些区块有默认底色，这里也一起覆盖 */
    .hero, .section, .footer { background: #fefefe; }
  </style>
</head>
<body onload="updateInTheWild();updateBimanual();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="title-video" aria-label="VPP-TC title loop">
            <video id="title-loop" autoplay muted loop playsinline poster="media/title-poster.jpg">
              <source src="media/videos/Title.mov" type="video/mp4">
              <!-- <source src="media/videos/title-loop.webm" type="video/webm"> -->
              Your browser does not support the video tag.
            </video>
          </div>
          <h1 class="title is-1 publication-title">VPP-TC: Viability-Preserving<br>Passive Torque Control</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">
              <a target="_blank" href=".">Anonymous Author(s)</a>
            </span>
            <!-- <span class="author-block">
              <a target="_blank" href="https://huangwl18.github.io/">Wenlong Huang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.chenwangjeremy.net/">Chen Wang</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://yunzhuli.github.io/">Yunzhu Li</a><sup>2*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://profiles.stanford.edu/fei-fei-li">Li Fei-Fei</a><sup>1</sup>
            </span> -->
          </div>
          <!-- <div class="is-size-5 affiliation">
            <sup>1</sup>Stanford University,
            <sup>2</sup>Columbia University
          </div>
          <br>
          <div class="affiliation-note">
            <sup>*</sup> indicates equal contributions
          </div> -->
          <div class="button-container">
            <a href="./cs.html" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="./cs.html" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="./cs.html" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a>
            <!-- <a href="https://x.com/wenlong_huang/status/1829135436717142319" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a> -->
            <a href="./cs.html" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  

<div class="container is-max-widescreen">
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Abstract</h2>
    <div class="content has-text-justified">
      <p>
        Conventional passivity-based torque controllers for manipulators are typically unconstrained, which can lead to safety violations under external perturbations. In this paper, we employ viability theory to pre-compute safe sets in the state-space of joint positions and velocities. These viable sets, constructed via data-driven and analytical methods for self-collision avoidance, external object collision avoidance and joint-position and joint-velocity limits, provide constraints on joint accelerations and thus joint torques via the robot dynamics. A quadratic programming-based control framework enforces these constraints on a passive controller tracking a dynamical system, ensuring the robot states remain within the safe set in an infinite time horizon. We validate the proposed approach through simulations and hardware experiments on a 7-DoF Franka Emika manipulator. In comparison to a baseline constrained passive controller, our method operates at higher control-loop rates and yields smoother trajectories.
      </p>
    </div>
  </div>
</div>

<div class="container is-max-widescreen">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-vcentered  is-centered">
        <video id="teaser" autoplay muted loop controls height="100%" width="100%">
          <source src="media/videos/VPP-TC.mp4"
                  type="video/mp4">
        </video>
        </br>
      </div>
      <br>
      <!-- <h2 class="subtitle has-text-centered">
      Large vision models and vision-language models can generate keypoint-based constraints, which can be optimized to achieve multi-stage, in-the-wild, bimanual, and reactive behaviors, without task-specific training or environment models.
      </h2> -->
    </div>
  </div>
</div>

<!-- <hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Walkthrough Video</h2>
  <div class="publication-video">
      <iframe src="https://www.youtube.com/embed/2S8YhBdLdww" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  </div>
</div>


<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Overview of ReKep</h2>
  <img src="media/figures/method.jpg" class="method-image" />
  <p class="content has-text-justified">Given RGB-D observation and free-form language instruction, DINOv2
    is used to propose keypoint candidates on fine-grained meaningful regions in the scene. The image overlaid
    with keypoints and the instruction are fed into GPT-4o to generate a series of ReKep constraints as python
    programs that specify desired relations between keypoints at different stages of the task and any
    requirement on the transitioning behaviors. Finally, a constrained optimization solver is used to obtain a
    dense sequence of end-effector actions in SE(3), subject to the generated constraints.
    <b>The entire pipeline does not involve any additional training or any task-specific data.</b>
  </p>
</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Closed-Loop Replanning across Multiple Stages</h2>

  <p class="content has-text-justified">Since keypoints are tracked in real-time, the system can replan its actions in closed loop, both <i>within stages</i> and <i>across stages</i>.
    Here, the operator randomly perturbs the objects and the robot, but the robot can quickly react to it.
    Note that after the robot tilts the teapot, if at this moment the cup is moved away from the robot, it would restore the teapot to be upright and attempt the pouring action again.
  Real-time solution is visualized on the right.</p>
  
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="media/videos/pour-tea-dist.m4v" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>

<hr class="rounded">

<div class="rows">
  <h2 class="title is-3">Bimanual Manipulation</h2>
  <p class="content has-text-justified">
    Bimanual tasks investigated in the paper. Select a task to see its video and solution visualization.
  </p>
    <div class="columns">
      <div class="column has-text-centered">
        <div class="select is-rounded">     
          <select id="bimanual-video-menu" onchange="updateBimanual()">
          <option value="pack-shoes" selected="selected">Pack Shoes</option>
          <option value="collaborative-folding">Collaborative Folding</option>
          <option value="fold-sweater">Fold Sweater</option>
          </select>
        </div>
      </div>
    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="bimanual-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/pour-tea-dist.m4v" type="video/mp4">
          </video>
        </p>
      </div>
    </div>
</div>

<hr class="rounded">

<div class="rows">
  <h2 class="title is-3">In-The-Wild Manipulation</h2>
  <p class="content has-text-justified">
    In-the-wild tasks investigated in the paper. Select a task to see its video and solution visualization.
  </p>
    <div class="columns">
      <div class="column has-text-centered">
        <div class="select is-rounded">     
          <select id="inthewild-video-menu" onchange="updateInTheWild()">
            <option value="stow-book" selected="selected">Stow Book</option>
          <option value="pour-tea">Pour Tea</option>
          <option value="tape-box">Tape Box</option>
          <option value="recycle-can">Recycle Can</option>
          </select>
        </div>
      </div>

    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="inthewild-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/pour-tea-dist.m4v" type="video/mp4">
          </video>
        </p>
      </div>
    </div>
</div>



<hr class="rounded">

<div class="rows">
  <h2 class="title is-3">Folding Clothes with Novel Strategies</h2>
    <p class="content has-text-justified">
      The system can also generate novel strategies for the same task but under different scenarios.
      Specifically, we investigate whether the same system can fold different types of clothing items.
      Interestingly, we observe drastically different strategies across the clothing categories, many of which align with how humans might fold each garment.
      Select a garment to see its folding strategy and its video.
      The coloring of the keypoints indicates the folding order, where red keypoints are aligned first and the blue keypoints are aligned subsequently.
    </p>
    <div class="columns">
      <div class="column has-text-centered">
        <div class="select is-rounded">     
          <select id="clothes-video-menu" onchange="updateClothes()">
            <option value="sweater" selected="selected">Sweater</option>
          <option value="shirt">Shirt</option>
          <option value="hoodie">Hoodie</option>
          <option value="vest">Vest</option>
          <option value="dress">Dress</option>
          <option value="pants">Pants</option>
          <option value="shorts">Shorts</option>
          <option value="scarf">Scarf</option>
          </select>
        </div>
      </div>

    </div>

    <div class="columns">
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <img id="clothes-img" width="60%" src="media/fold-strategies/sweater.jpeg" class="method-image" />
        </p>
      </div>
      <div class="column has-text-centered">
        <p style="text-align:center;">
          <video id="clothes-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/fold-sweater.mp4" type="video/mp4">
          </video>
        </p>
      </div>
    </div>
</div>

<hr class="rounded">
<h2 class="title is-3">Acknowledgments</h2>
<p>
  This work is partially supported by Stanford Institute for Human-Centered Artificial Intelligence, ONR MURI N00014-21-1-2801, and Schmidt Sciences. Ruohan Zhang is partially supported by Wu Tsai Human Performance Alliance Fellowship. The bimanual hardware is partially supported by Stanford TML. We would like to thank the anonymous reviewers, Albert Wu, Yifan Hou, Adrien Gaidon, Adam Harley, Christopher Agia, Edward Schmerling, Marco Pavone, Yunfan Jiang, Yixuan Wang, Sirui Chen, Chengshu Li, Josiah Wong, Wensi Ai, Weiyu Liu, Mengdi Xu, Yihe Tang, Chelsea Ye, Mijiu Mili, and the members of the Stanford Vision and Learning Lab for fruitful discussions, helps on experiments, and support.
</p>
<hr class="rounded">
<h2 class="title is-3">BibTeX</h2>
<p class="bibtex">
    @article{huang2024rekep, <br>
    &nbsp;&nbsp;title = {ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation}, <br>
    &nbsp;&nbsp;author = {Huang, Wenlong and Wang, Chen and Li, Yunzhu and Zhang, Ruohan and Fei-Fei, Li}, <br>
    &nbsp;&nbsp;journal = {arXiv preprint arXiv:2409.01652}, <br>
    &nbsp;&nbsp;year = {2024} <br>
    }
</p>

</section>
</div> -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://rekep-robot.github.io/">ReKep</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
